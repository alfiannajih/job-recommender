input_dir: 'dataset/resume_dataset'
#Training
seed: 0
learning_rate: 1e-5
weight_decay: 0.05
patience: 2
batch_size: 1
grad_steps: 2

# Learning Rate Scheduler
num_epochs: 2
warmup_epochs: 1

# Validation
eval_batch_size: 1

# LLM related
llm_model_path: "google/gemma-2-2b-it" #"TinyLlama/TinyLlama_v1.1" #'meta-llama/Llama-2-7b-hf'
llm_frozen: 'True'
llm_num_virtual_tokens: 10
output_dir: 'output'
max_txt_len: 1024
max_new_tokens: 256

# GNN related
gnn_num_layers: 4
gnn_in_dim: 768
gnn_hidden_dim: 1024
gnn_num_heads: 4
gnn_dropout: 0.0