input_dir: 'dataset/resume_dataset'
#Training
seed: 0
learning_rate: 1e-5
weight_decay: 0.05
patience: 2
batch_size: 1
grad_steps: 2

# Learning Rate Scheduler
num_epochs: 3
warmup_epochs: 1

# Validation
eval_batch_size: 1

# LLM related
llm_model_path: "TinyLlama/TinyLlama_v1.1" #'meta-llama/Llama-2-7b-hf'
llm_frozen: 'True'
llm_num_virtual_tokens: 10
output_dir: "/gcs/compfest" #'output'
max_txt_len: 1024
max_new_tokens: 512

# GNN related
gnn_num_layers: 4
gnn_in_dim: 768
gnn_hidden_dim: 1024
gnn_num_heads: 4
gnn_dropout: 0.0